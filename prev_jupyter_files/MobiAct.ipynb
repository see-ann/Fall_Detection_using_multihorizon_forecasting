{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T05:10:30.034884Z",
     "start_time": "2021-06-03T05:10:17.077439Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jg/47v2cmwn6bx5mnmlt0lbg_pw0000gn/T/ipykernel_12835/104573692.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T05:10:30.078085Z",
     "start_time": "2021-06-03T05:10:30.040125Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m device\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-03T05:10:33.684725Z",
     "start_time": "2021-06-03T05:10:33.679222Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../dataset/mobiact_preprocessed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T06:41:51.607926Z",
     "start_time": "2021-06-01T06:41:07.371211Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_dir + 'train.csv')\n",
    "valid = pd.read_csv(data_dir + 'valid.csv')\n",
    "test = pd.read_csv(data_dir + 'test.csv')\n",
    "activity_info = pd.read_csv(data_dir + 'activity_info.csv')\n",
    "\n",
    "\n",
    "valid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T06:41:55.615951Z",
     "start_time": "2021-06-01T06:41:51.613265Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(activity_info['Label'])\n",
    "\n",
    "train_encoded = encoder.transform(train['label'])\n",
    "train['label_encoded'] = train_encoded\n",
    "\n",
    "valid_encoded = encoder.transform(valid['label'])\n",
    "valid['label_encoded'] = valid_encoded\n",
    "\n",
    "test_encoded = encoder.transform(test['label'])\n",
    "test['label_encoded'] = test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T06:42:01.325470Z",
     "start_time": "2021-06-01T06:41:55.618740Z"
    }
   },
   "outputs": [],
   "source": [
    "train_obs_scaler = StandardScaler()\n",
    "train_tar_scaler = StandardScaler()\n",
    "\n",
    "test_obs_scaler = StandardScaler()\n",
    "test_tar_scaler = StandardScaler()\n",
    "\n",
    "columns = ['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z','label', 'person_id']\n",
    "\n",
    "obs_train = train[['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']]\n",
    "obs_train = train_obs_scaler.fit_transform(obs_train)\n",
    "tar_train = np.asarray(train['label_encoded'])\n",
    "tar_train = train_tar_scaler.fit_transform(tar_train.reshape(-1, 1))\n",
    "\n",
    "obs_train = pd.DataFrame(obs_train)\n",
    "tar_train = pd.DataFrame(tar_train)\n",
    "transformed_train = pd.concat([obs_train, tar_train], axis=1)\n",
    "transformed_train['per-id'] = train['person_id'].values\n",
    "transformed_train.columns = columns\n",
    "# transformed_train.head()\n",
    "\n",
    "obs_valid = valid[['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']]\n",
    "obs_valid = train_obs_scaler.fit_transform(obs_valid)\n",
    "tar_valid = np.asarray(valid['label_encoded'])\n",
    "tar_valid = train_tar_scaler.fit_transform(tar_valid.reshape(-1, 1))\n",
    "\n",
    "obs_valid = pd.DataFrame(obs_valid)\n",
    "tar_valid = pd.DataFrame(tar_valid)\n",
    "transformed_valid = pd.concat([obs_valid, tar_valid], axis=1)\n",
    "transformed_valid['per-id'] = valid['person_id'].values\n",
    "transformed_valid.columns = columns\n",
    "# transformed_valid.head()\n",
    "\n",
    "obs_test = test[['acc_x', 'acc_y', 'acc_z', 'gyro_x', 'gyro_y', 'gyro_z']]\n",
    "obs_test = test_obs_scaler.fit_transform(obs_test)\n",
    "tar_test = np.asarray(test['label_encoded'])\n",
    "tar_test = test_tar_scaler.fit_transform(tar_test.reshape(-1, 1))\n",
    "\n",
    "obs_test = pd.DataFrame(obs_test)\n",
    "tar_test = pd.DataFrame(tar_test)\n",
    "transformed_test = pd.concat([obs_test, tar_test], axis=1)\n",
    "transformed_test['per-id'] = test['person_id'].values\n",
    "transformed_test.columns = columns\n",
    "\n",
    "transformed_test = transformed_test.sort_values(by='person_id')\n",
    "transformed_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b38c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_train.sort_values(by='person_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T06:42:01.347883Z",
     "start_time": "2021-06-01T06:42:01.330973Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataProcessing(data):\n",
    "    X_data, y_data = list(), list()\n",
    "    for i in range(43, len(data) - 44):\n",
    "        acc_x = data['acc_x'][i-43: i]\n",
    "        acc_y = data['acc_y'][i-43: i]\n",
    "        acc_z = data['acc_z'][i-43: i]\n",
    "        gyro_x = data['gyro_x'][i-43: i]\n",
    "        gyro_y = data['gyro_y'][i-43: i]\n",
    "        gyro_z = data['gyro_z'][i-43: i]\n",
    "        X_data.append([acc_x.values, acc_y.values, acc_z.values, gyro_x.values, gyro_y.values, gyro_z.values])\n",
    "\n",
    "        outcome = data['label'][i: i+44]\n",
    "        y_data.append([outcome.values])\n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-01T06:41:14.755Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = dataProcessing(transformed_train)\n",
    "X_valid, y_valid = dataProcessing(transformed_valid)\n",
    "X_test, y_test = dataProcessing(transformed_test)\n",
    "\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-01T04:43:09.768Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_data = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "valid_data = TensorDataset(torch.Tensor(X_valid), torch.Tensor(y_valid))\n",
    "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb96ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:03:19.248664Z",
     "start_time": "2021-06-01T02:03:19.239938Z"
    }
   },
   "outputs": [],
   "source": [
    "from model.singleLSTM import singleLSTM\n",
    "from model.stackedLSTM import stackedLSTM\n",
    "from model.CNN import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:03:19.257414Z",
     "start_time": "2021-06-01T02:03:19.250670Z"
    }
   },
   "outputs": [],
   "source": [
    "def choose_model(model_type):\n",
    "    if model_type == 'singleLSTM':\n",
    "        model = singleLSTM(3, 320, 32).cuda()\n",
    "        save_path = 'results/singleLSTM/'\n",
    "        save_file_name = save_path + 'SingleLSTMNotch.pth'\n",
    "    elif model_type == 'stackedLSTM':\n",
    "        model = stackedLSTM(3, 320, 32).cuda()\n",
    "        save_path = 'results/stackedLSTM/'\n",
    "        save_file_name = save_path + 'StackedLSTMNotch.pth'\n",
    "    elif model_type == 'CNN':\n",
    "        model = CNN(3, [8, 16, 64], 32).cuda()\n",
    "        save_path = 'results/CNN/'\n",
    "        save_file_name = save_path + 'CNNNotch.pth'\n",
    "    return model, save_path, save_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:11:31.679374Z",
     "start_time": "2021-06-01T02:11:31.637821Z"
    }
   },
   "outputs": [],
   "source": [
    "model_type = 'singleLSTM'\n",
    "model, save_path, save_file_name = choose_model(model_type)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:11:33.201682Z",
     "start_time": "2021-06-01T02:11:33.188069Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                       mode='min',\n",
    "                                                       factor=0.2\n",
    "                                                    )\n",
    "criterion = nn.MSELoss().cuda()\n",
    "patience = 5\n",
    "n_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:11:34.083895Z",
     "start_time": "2021-06-01T02:11:34.061972Z"
    }
   },
   "outputs": [],
   "source": [
    "# example\n",
    "\n",
    "with torch.no_grad():\n",
    "    dataiter = iter(train_loader)\n",
    "    X, y = dataiter.next()\n",
    "\n",
    "    ex_model = model\n",
    "    if model_type != 'CNN':\n",
    "        X = X.view(-1, 32, 3)\n",
    "    print('X size', X.shape)\n",
    "    print('y size', y.shape)\n",
    "    output = ex_model(X.cuda())\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:03:22.297201Z",
     "start_time": "2021-06-01T02:03:22.279481Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit(model, batch_size, n_epochs, save_file_name):\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "\n",
    "    best_loss = 9999999999\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            if model_type != 'CNN':\n",
    "                data, target = data.view(-1, 32, 3).cuda(), target.cuda()\n",
    "            else:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            if model_type != 'CNN':\n",
    "                loss = criterion(output, target.squeeze())\n",
    "            else:\n",
    "                loss = criterion(output.squeeze(), target.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################\n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for data, target in valid_loader:\n",
    "            if model_type != 'CNN':\n",
    "                data, target = data.view(-1, 32, 3).cuda(), target.cuda()\n",
    "            else:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data.cuda())\n",
    "            if model_type != 'CNN':\n",
    "                loss = criterion(output, target.squeeze())\n",
    "            else:\n",
    "                loss = criterion(output.squeeze(), target.squeeze())\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "\n",
    "        epoch_len = len(str(n_epochs))\n",
    "\n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "\n",
    "        print(print_msg)\n",
    "\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            patience = 0\n",
    "            torch.save(model.state_dict(), save_file_name)\n",
    "            print('Saving Model')\n",
    "        else:\n",
    "            patience += 1\n",
    "            print('Patience for ', patience)\n",
    "        if patience == 10:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load(save_file_name))\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:07:30.186827Z",
     "start_time": "2021-06-01T02:03:22.299508Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, train_loss, valid_loss = fit(model, batch_size, n_epochs, save_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:11:37.380117Z",
     "start_time": "2021-06-01T02:11:37.362099Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    test_loss = 0.0\n",
    "    y_test = []\n",
    "    y_hat = []\n",
    "    model.eval()\n",
    "    for data,target in test_loader:\n",
    "        if model_type != 'CNN':\n",
    "            data, target = data.view(-1, 32, 3).cuda(), target.cuda()\n",
    "        else:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data.cuda())\n",
    "        loss = criterion(output.squeeze(), target.squeeze())\n",
    "        test_loss += loss.item()\n",
    "        y_test += list(target.squeeze().detach().cpu().numpy())\n",
    "        y_hat += list(output.squeeze().detach().cpu().numpy())\n",
    "\n",
    "    return test_loss/len(test_loader), y_test, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:11:38.335440Z",
     "start_time": "2021-06-01T02:11:38.302393Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path + 'SingleLSTMNotch.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:11:54.199316Z",
     "start_time": "2021-06-01T02:11:39.779847Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loss, y_target, y_hat = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:11:54.282903Z",
     "start_time": "2021-06-01T02:11:54.201981Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = test_tar_scaler.inverse_transform(y_target)\n",
    "predicted = test_tar_scaler.inverse_transform(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:11:54.299017Z",
     "start_time": "2021-06-01T02:11:54.290238Z"
    }
   },
   "outputs": [],
   "source": [
    "len(y_target), len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:11:55.046061Z",
     "start_time": "2021-06-01T02:11:54.302984Z"
    }
   },
   "outputs": [],
   "source": [
    "corrected_labels = []\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] != 1.0:\n",
    "        corrected_labels.append(0.0)\n",
    "    else:\n",
    "        corrected_labels.append(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:11:55.377190Z",
     "start_time": "2021-06-01T02:11:55.048216Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt_y_hat = Counter(predicted)\n",
    "\n",
    "thres = np.array(list(cnt_y_hat.keys())).mean()\n",
    "corrected_predicted = []\n",
    "for i in range(len(predicted)):\n",
    "    if predicted[i] < thres:\n",
    "        corrected_predicted.append(0.0)\n",
    "    else:\n",
    "        corrected_predicted.append(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:11:56.893077Z",
     "start_time": "2021-06-01T02:11:55.379899Z"
    }
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(corrected_labels, corrected_predicted))\n",
    "print()\n",
    "acc = accuracy_score(corrected_labels, corrected_predicted)\n",
    "f1 = f1_score(corrected_labels, corrected_predicted)\n",
    "recall = recall_score(corrected_labels, corrected_predicted)\n",
    "precision = precision_score(corrected_labels, corrected_predicted)\n",
    "\n",
    "print('recall score is ', recall)\n",
    "print('precision score is ', precision)\n",
    "print('f1 score is ', f1)\n",
    "print('accuracy score is ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:07:52.906594Z",
     "start_time": "2021-06-01T02:07:52.881774Z"
    }
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame([acc, precision, recall, f1]).T\n",
    "result.columns = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "result.to_csv(save_path + 'stackedLSTMNotch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:11:58.308740Z",
     "start_time": "2021-06-01T02:11:58.192545Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_32, predicted_32 = list(), list()\n",
    "for i in range(len(labels)):\n",
    "    if i % 32 == 0:\n",
    "        labels_32.append(labels[i])\n",
    "for i in range(len(predicted)):\n",
    "    if i % 32 == 0:\n",
    "        predicted_32.append(predicted[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-01T02:11:59.154735Z",
     "start_time": "2021-06-01T02:11:58.927351Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(labels_32[600:1600], label='target', color='darkorange')\n",
    "plt.plot(predicted_32[600:1600], label='predicted', color='hotpink')\n",
    "plt.legend(loc='upper left', fontsize=15)\n",
    "plt.show()\n",
    "plt.savefig(save_path + 'singleLSTMNotch.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
